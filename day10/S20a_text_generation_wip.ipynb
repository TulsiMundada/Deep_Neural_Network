{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks \n",
    "## Session 20a : Lecture\n",
    "\n",
    "## Text Generation using RNN\n",
    "<img src='../../prasami_images/prasami_color_tutorials_small.png' style = 'width:400px;' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets import some libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_verify_dir(_path):\n",
    "    '''\n",
    "    Arg:\n",
    "        path: path to verify the directory\n",
    "    returns:\n",
    "        create dir if it does not exists\n",
    "    '''\n",
    "    if os.path.exists(_path): # check if the path exists. Maybe a file or a folder\n",
    "        \n",
    "        print(_path, ' exists') # advised the user\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        os.makedirs(_path) # create the path\n",
    "        \n",
    "        print(\"Created folder : \", _path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "modelDir = '../../models' # location to store models\n",
    "subDir = 'text_gen' # location to store models\n",
    "\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EPOCHS = 50 # number of cycles to run\n",
    "\n",
    "ALPHA = 0.1 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../input\\\\text_gen\\\\shakespeare.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filePath = os.path.join(inpDir, subDir, 'shakespeare.txt')\n",
    "filePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open(filePath, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "len(text)\n",
    "\n",
    "#tf.io.read_file(filePath).numpy()..decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it \n"
     ]
    }
   ],
   "source": [
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115395,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)} # \n",
    "#creat indexing of vocab\n",
    "\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, ...,  8,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115395,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1.,2.,3.])\n",
    "\n",
    "print (list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 | F\n",
      "47 | i\n",
      "56 | r\n",
      "57 | s\n",
      "58 | t\n",
      "1 |  \n",
      "15 | C\n",
      "47 | i\n",
      "58 | t\n",
      "47 | i\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "example_per_epoch = len(text) // (seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    \n",
    "    print (i.numpy(), '|', idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(2):\n",
    "    \n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(2):\n",
    "    \n",
    "    print (repr( ''.join(idx2char[item.numpy()] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \n",
    "    input_text = chunk[:-1]\n",
    "    \n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "************************************************** \n",
      "\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "'re all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "************************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_ex, tar_ex in dataset.take (2):\n",
    "    print (repr( ''.join(idx2char[inp_ex.numpy()] ) ))\n",
    "    print (repr( ''.join(idx2char[tar_ex.numpy()] ) ))\n",
    "    print ('*'*50, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Embedding(vocab_size, \n",
    "                                  embedding_dim, \n",
    "                                  batch_input_shape= [batch_size, None]),\n",
    "        \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True, \n",
    "                            stateful=True, \n",
    "                            recurrent_initializer='glorot_uniform'\n",
    "                           ),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size, embedding_dim, rnn_units, batch_size\n",
    "model = build_model(vocab_size= len(vocab), \n",
    "                    embedding_dim=embedding_dim, \n",
    "                    rnn_units = rnn_units,\n",
    "                    batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_ex_batch, target_ex_batch in dataset.take(1):\n",
    "    ex_batch_pred = model(input_ex_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 100, 65])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_batch_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4021569 (15.34 MB)\n",
      "Trainable params: 4021569 (15.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 36,  2, 64, 51,  0, 12,  8, 64,  7, 27, 55, 57, 22, 49, 38, 57,\n",
       "       41, 59, 11, 59, 11, 17, 12, 21, 63, 55, 47, 31, 53, 29, 30,  4, 35,\n",
       "        7,  1, 28, 36, 39,  8, 36, 25, 27,  3, 44, 32, 27, 41, 29, 63, 49,\n",
       "       60, 25, 14, 64, 21, 13, 62, 22, 24, 25, 19, 15,  3, 31,  4,  6, 27,\n",
       "       42, 39, 34, 11, 29, 17, 13, 36, 51, 24, 40,  1, 48, 56, 47,  6, 54,\n",
       "       18, 26, 33, 37, 34, 33, 56, 40, 42, 52,  0, 24, 45, 54, 38],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(ex_batch_pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()\n",
    "\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkPtPath = os.path.join(modelDir, subDir)\n",
    "\n",
    "chkPtPrefix = os.path.join(chkPtPath, 'chkpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=chkPtPrefix,\n",
    "                                                        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 2.6945\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 172s 993ms/step - loss: 1.9624\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.6919\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 172s 990ms/step - loss: 1.5435\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 173s 996ms/step - loss: 1.4550\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 178s 1s/step - loss: 1.3953\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.3493\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.3111\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 1.2765\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.2429\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.2116\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.1781\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.1464\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.1125\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.0763\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.0408\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 1.0047\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.9684\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.9351\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.9002\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.8689\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 177s 1s/step - loss: 0.8390\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.8137\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.7881\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 173s 998ms/step - loss: 0.7663\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.7476\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.7303\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.7178\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.7061\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6956\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6863\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.6759\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6706\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 173s 1000ms/step - loss: 0.6627\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.6600\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.6544\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.6505\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6489\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6448\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 174s 1s/step - loss: 0.6410\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 181s 1s/step - loss: 0.6399\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 182s 1s/step - loss: 0.6414\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 183s 1s/step - loss: 0.6377\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 178s 1s/step - loss: 0.6374\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 179s 1s/step - loss: 0.6377\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 182s 1s/step - loss: 0.6366\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.6352\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 173s 999ms/step - loss: 0.6358\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.6364\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 175s 1s/step - loss: 0.6392\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../models\\\\text_gen\\\\chkpt_50'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(chkPtPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(chkPtPath))\n",
    "\n",
    "model.build ( tf.TensorShape ( [1, None ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4021569 (15.34 MB)\n",
      "Trainable params: 4021569 (15.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    num_generate =  1000\n",
    "    input_eval = [char2idx[s] for s in start_string] # [37, 48, 56 ]\n",
    "    print (f'Input: {start_string} | {input_eval}\\n')\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # tf.Tensor (1, 1, 5)\n",
    "    text_generated = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predict_td = tf.random.categorical(predictions, \n",
    "                                            num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predict_td], 0)\n",
    "        text_generated.append(idx2char[predict_td])\n",
    "        \n",
    "    return start_string+''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ROMEO: | [30, 27, 25, 17, 27, 10]\n",
      "\n",
      "ROMEO:\n",
      "The begging.\n",
      "\n",
      "VOLUNNA:\n",
      "Now of our will, sit faults\n",
      "Before I have told this firmle hurt not; that it may nay\n",
      "Young Romeo let pale counsel!\n",
      "The Expose is misleigh,\n",
      "To offerce well for Rome.\n",
      "\n",
      "First Soldier:\n",
      "I shall, sir. Fare you well.\n",
      "\n",
      "BAUNT:\n",
      "Come, come, tready met I speak blown with the bone.\n",
      "But what of him?\n",
      "\n",
      "BIONDELLO:\n",
      "Where lies hard,\n",
      "Which sorrow wither.\n",
      "\n",
      "SAMPSON:\n",
      "Great lord, myself, and that.\n",
      "\n",
      "BENVOLIO:\n",
      "Say, what! get thee my lord?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Great Aufeous slave,\n",
      "From our freech-beauteful Clarence,\n",
      "Our darges banish'd and faster to thy dagger toward that names' twenty you will go whis ended question.\n",
      "My soul.\n",
      "\n",
      "Third Servingman:\n",
      "Who might have kept that name is omis, his lady may speak\n",
      "Of what you lose her that would mus apactor?\n",
      "Thou foot may please the house of Lancaster.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Welcome, sweet house?\n",
      "\n",
      "KATHARINA:\n",
      "Part of merity.\n",
      "\n",
      "GREMIO:\n",
      "Adieu, good fellows; but I cannot meet him; he hath\n",
      "After the case of Longord? But\n",
      "he would do, Princes off straight,\n",
      "Unless you partly m\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(model, start_string=u'ROMEO:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
